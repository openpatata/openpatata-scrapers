
from datetime import date
import itertools
import logging
import re

from scrapers.crawling import Task
from scrapers.misc_utils import starfilter
from scrapers import records
from scrapers.tasks.plenary_agendas import (extract_parliamentary_period,
                                            extract_session)
from scrapers.text_utils import (apply_subs, CanonicaliseName, clean_spaces,
                                 date2dato, parse_long_date, TableParser)

logger = logging.getLogger(__name__)


class PlenaryTranscripts(Task):
    """Extract MPs in attendance at plenary sittings from transcripts."""

    async def process_transcript_listings(self):
        listing_urls = (
            'http://www2.parliament.cy/parliamentgr/008_01_01/008_01_IA.htm',
            'http://www2.parliament.cy/parliamentgr/008_01_01/008_01_IB.htm',
            'http://www2.parliament.cy/parliamentgr/008_01_01/008_01_IES.htm',
            'http://www2.parliament.cy/parliamentgr/008_01_01/008_01_IC.htm',
            'http://www2.parliament.cy/parliamentgr/008_01_01/008_01_IDS.htm',
            'http://www2.parliament.cy/parliamentgr/008_01_01/008_01_ID.htm',
            'http://www2.parliament.cy/parliamentgr/008_01_01/008_01_IE.htm')

        transcript_urls = await \
            self.c.gather({self.process_transcript_listing(url)
                           for url in listing_urls})
        transcript_urls = itertools.chain.from_iterable(transcript_urls)
        return await self.c.gather({self.process_transcript(url)
                                    for url in transcript_urls})

    __call__ = process_transcript_listings

    async def process_transcript_listing(self, url):
        html = await self.c.get_html(url)
        return html.xpath('//a[contains(@href, "praktiko")]/@href')

    async def process_transcript(self, url):
        return url, await self.c.get_payload(url, decode=True)

    @staticmethod
    def after(output):
        for transcript in output:
            _parse_transcript(*transcript)


def _extract_attendee_name(url, name):
    if name.isdigit():
        return      # Skip page numbers

    new_name = CanonicaliseName.from_garbled(
        clean_spaces(name, medial_newlines=True))
    if not new_name:
        logger.warning('Unable to pair name {!r} with MP on record'
                       ' while parsing {!r}'.format(name, url))
        return
    if name != new_name:
        logger.info('Name {!r} converted to {!r}'
                    ' while parsing {!r}'.format(name, new_name, url))
    return new_name


PRESIDENTS = (((date(2001, 6, 21), date(2007, 12, 19)), 'Î§ÏÎ¹ÏƒÏ„ÏŒÏ†Î¹Î±Ï‚ Î”Î·Î¼Î®Ï„ÏÎ·Ï‚'),
              ((date(2008, 3, 20), date(2011, 4, 22)), 'ÎšÎ±ÏÎ¿Î³Î¹Î¬Î½ ÎœÎ¬ÏÎ¹Î¿Ï‚'),
              ((date(2011, 6, 9), date.today()), 'ÎŸÎ¼Î®ÏÎ¿Ï… Î“Î¹Î±Î½Î½Î¬ÎºÎ·Ï‚')
              )
PRESIDENTS = tuple((range(*map(date.toordinal, dates)), {name})
                   for dates, name in PRESIDENTS)


def _select_president(date_):
    match = starfilter((lambda date: lambda date_range, _: date in date_range)
                       (date2dato(date_).toordinal()), PRESIDENTS)
    try:
        _, name = next(match)
        return name
    except StopIteration:
        raise ValueError('No President found for {!r}'.format(date_)) from None


SUBS = (('|', ''),
        ('Î Î±ÏÏŒÎ½Ï„ÎµÏ‚ Î²Î¿Ï…Î»ÎµÏ…Ï„Î­Ï‚', 'ğŸŒ®'),
        ('Î Î‘Î¡ÎŒÎÎ¤Î•Î£ Î’ÎŸÎ¥Î›Î•Î¥Î¤ÎˆÎ£', 'ğŸŒ®'),      # pandoc
        ('(ÎÏÎ± Î»Î®Î¾Î·Ï‚: 6.15 Î¼.Î¼.)', 'ğŸŒ®'),  # 2015-03-19
        ('Î Î±ÏÏŒÎ½Ï„ÎµÏ‚ Î±Î½Ï„Î¹Ï€ÏÏŒÏƒÏ‰Ï€Î¿Î¹ Î¸ÏÎ·ÏƒÎºÎµÏ…Ï„Î¹ÎºÏÎ½ Î¿Î¼Î¬Î´Ï‰Î½', 'ğŸŒ¯'),
        ('Î‘Î½Ï„Î¹Ï€ÏÏŒÏƒÏ‰Ï€Î¿Î¹ Î¸ÏÎ·ÏƒÎºÎµÏ…Ï„Î¹ÎºÏÎ½ Î¿Î¼Î¬Î´Ï‰Î½', 'ğŸŒ¯'),  # 2014-10-23
        ('Î ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î±', 'ğŸŒ¯'),
        ('Î Î•Î¡Î™Î•Î§ÎŸÎœÎ•ÎÎ‘', 'ğŸŒ¯'),
        ('Î¦Î±ÎºÎ¿Î½Ï„Î®Ï‚ Î£Î¿Ï†Î¿ÎºÎ»Î®Ï‚', 'Î¦Î±ÎºÎ¿Î½Ï„Î®Ï‚ Î‘Î½Ï„ÏÎ­Î±Ï‚')  # s/e in 2011-12-13
        )


def _extract_attendees(url, text, heading, date):
    # Split at page breaks 'cause the columns will have likely shifted
    # and strip off leading whitespace
    attendee_table = (apply_subs(text, SUBS).rpartition('ğŸŒ®')[2]
                                            .partition('ğŸŒ¯')[0])
    attendee_table = attendee_table.split('\x0c')
    attendee_table = ('\n'.join(l.lstrip() for l in s.splitlines())
                      for s in attendee_table)

    attendees = set(filter(None, (_extract_attendee_name(url, a)
                                  for t in attendee_table
                                  for a in TableParser(t, max_cols=2).values)))
    # The President's not listed among the attendees
    attendees = attendees | _select_president(date) if 'Î Î¡ÎŸÎ•Î”Î¡ÎŸÎ£:' in heading \
        else attendees
    attendees = sorted(attendees)
    return attendees


RE_SI = re.compile(r'Î‘Ï\. (\d+)')


def _extract_sitting(url, text):
    match = RE_SI.search(text)
    return int(match.group(1)) if match else logger.warning(
        'Unable to extract sitting number of {!r}'.format(url))


def _parse_transcript(url, text):
    heading = clean_spaces(text.partition('(')[0], medial_newlines=True)
    try:
        date = parse_long_date(heading)
    except ValueError as e:
        logger.error('{} in {!r}'.format(e, url))
        return

    plenary_sitting = records.PlenarySitting.from_template(
        sources=(url,),
        update={'attendees': _extract_attendees(url, text, heading, date),
                'date': date,
                'links': [{'type': 'transcript', 'url': url}],
                'parliamentary_period': extract_parliamentary_period(url,
                                                                     heading),
                'session': extract_session(url, heading),
                'sitting': _extract_sitting(url, heading)})
    try:
        plenary_sitting.merge() if plenary_sitting.exists else \
            plenary_sitting.insert()
    except records.InsertError as e:
        logger.error(e)
